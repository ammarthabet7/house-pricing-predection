# -*- coding: utf-8 -*-
"""ammar's golden notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XPdSQw_2VN-Yv9FsqpHPigkv7gOxR0io

# importing essintial libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('/content/data.csv')

"""#  manipulation

**first ,boring essintial step ,lets view our data**
"""

df.head(25).T

"""quick description for the features

1) sqft: is an abbreviation for square feet for(living room ,loot room,.....)

2) waterfront :describes if the house is near to a view of water(sea,river)or not


3)condition:it is an essintial part in the contract between buyer and seller

4)grade:is the quality of how the building was built

5)above:the higher floors

6)basement:undergrund floor

7)lat,long: geographical information that indicates the location of the house

"""

#describtion of data
df.describe()

#consider the shape of data
df.shape

"""**we can make some calculations on columns to ease out analysis**"""

#calculating age from yr_built
df['yr_built']=2023-df['yr_built']

df['yr_built'].head()

"""**drop some unwanted features as they dont describe any thing**"""

df.drop(columns=['id','date'],axis =1, inplace =True)

df.head().T

"""**searching for columns that have meaningless values**"""

#bathrooms have some decimal values which is insane(after searching the decimals describes if the bathroom have shower or not but i decide to ignore this)
df['bathrooms'].head(10)

#rounding our features to get rid from decimals
df['bathrooms']=df['bathrooms'].round(decimals = 0)

df['bathrooms'].head(10)

"""**lets check for all columns which must have intiger numbers to not fall in "bathrooms column"mistake**"""

df['bedrooms'].unique()

df['floors'].unique()

"""**floors have decimal values lets approximate them**"""

df['floors']=df['floors'].round(decimals = 0)

df['floors'].unique()

"""**is there nulls?**"""

df.info()

df.isna().sum().sum()

"""â™¥

**we are not data science engineers if we dont look more deaply , lets see how is the data distributed**
"""

plt.figure(figsize=(20,35))
for i, col in enumerate(df.columns):
    if df[col].dtype != 'object':
        ax = plt.subplot(10, 2, i+1)
        sns.kdeplot(df[col], ax=ax)
        plt.xlabel(col)
        
plt.show()

df['sqft_lot15'].plot(kind = 'hist')

"""**sqft_lot15 (and some other features) is highly skewed , we shall try to solve this**"""

df.skew()

"""**as the acceptable value for skweness is between -3 and 3 so , (price,sqft_lot,waterfront,yr_renovated,sqft_lot15)are skewed ,lets solve this with square root transformation, but first check corelations**"""

#check correlation before applying trasformation
plt.figure(figsize = (30, 25))
sns.heatmap(df.corr(), annot = True, cmap="YlGnBu")
plt.show()

"""**all the 5 features have low correlations with the target y , lets apply square root normalization to heal skeweness and improve correlations.**"""

#we cannt use log trnsformation bocause '0' values will result to inf values in the dataframe
df['price']=np.sqrt(df['price'])

df['sqft_lot']=np.sqrt(df['sqft_lot'])

df['waterfront']=np.sqrt(df['waterfront'])

df['yr_renovated']=np.sqrt(df['yr_renovated'])

df['sqft_lot15']=np.sqrt(df['sqft_lot15'])

#scaling data due to very high ranges
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()

M = df['price'].to_numpy()
df['price']=scaler.fit_transform(M.reshape(-1,1))

M = df['sqft_lot'].to_numpy()
df['sqft_lot']=scaler.fit_transform(M.reshape(-1,1))

M = df['waterfront'].to_numpy()
df['waterfront']=scaler.fit_transform(M.reshape(-1,1))

M = df['yr_renovated'].to_numpy()
df['yr_renovated']=scaler.fit_transform(M.reshape(-1,1))

M = df['sqft_lot15'].to_numpy()
df['sqft_lot15']=scaler.fit_transform(M.reshape(-1,1))

#check that scaling is applied
df.head()

#Correlation using heatmap
plt.figure(figsize = (30, 25))
sns.heatmap(df.corr(), annot = True, cmap="YlGnBu")
plt.show()

df.skew()
#it improved

"""**here we are done with the skewness problem**

**before dropping weak correlated features ,check for outliers and apply scaling for the rest features**
"""

# Checking outliers
plt.figure(figsize= (70,45))
sns.boxplot(data=df,palette='rainbow',orient='h')

#lets cout them

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()

"""**they are a very large number so we cant remove them,only remove 'lat' outliers they are only 2 records**"""

def Remove_Outlier_Indices(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    trueList = ~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))
    return trueList

# Index List of Non-Outliers
nonOutlierList = Remove_Outlier_Indices(df['lat']) 
df = df[nonOutlierList]

#lets check

Q1 = df['lat'].quantile(0.25)
Q3 = df['lat'].quantile(0.75)
IQR = Q3 - Q1
((df['lat'] < (Q1 - 1.5 * IQR)) | (df['lat'] > (Q3 + 1.5 * IQR))).sum()

"""**we wont impute outliers because they truley have meaning (outliers in price means very expensive apartment,in square feet of any room means large room,.....) we can only normalize them**

**lets handel outliers**
"""

#we have a problem with(price,sqft_living,sqft_lot,sqft_above,sqft_basement,sqft_living15,sqft_lot15)
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()

M = df['sqft_living'].to_numpy()
df['sqft_living']=scaler.fit_transform(M.reshape(-1,1))


M = df['sqft_above'].to_numpy()
df['sqft_above']=scaler.fit_transform(M.reshape(-1,1))

M = df['sqft_basement'].to_numpy()
df['sqft_basement']=scaler.fit_transform(M.reshape(-1,1))

M = df['sqft_living15'].to_numpy()
df['sqft_living15']=scaler.fit_transform(M.reshape(-1,1))

"""**check again**"""

plt.figure(figsize= (70,45))
sns.boxplot(data=df,palette='rainbow',orient='h')

"""**it is done with outliers**"""

#scaling all features
scaler=MinMaxScaler()
M = df['yr_built'].to_numpy()
df['yr_built']=scaler.fit_transform(M.reshape(-1,1))

M = df['zipcode'].to_numpy()
df['zipcode']=scaler.fit_transform(M.reshape(-1,1))

M = df['lat'].to_numpy()
df['lat']=scaler.fit_transform(M.reshape(-1,1))

M = df['long'].to_numpy()
df['long']=scaler.fit_transform(M.reshape(-1,1))

df.head()

"""**by searching,it isnt trusted that using two kindes of scaling(square root and minmax scaler) is a right way but by trial it gives better accuracy value **

# modeling for prediction by polynomial regression
"""

#selecting price as a target
X=df.iloc[:,1:].values
y=df.iloc[:,0].values

#plotting the first two features
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))
x_1=df.iloc[:,1]
x_2=df.iloc[:,2]                     
axes[0].scatter(x_1, y)
axes[1].scatter(x_2, y)
axes[0].set_title("x_1 plotted")
axes[1].set_title("x_2 plotted")
plt.show()

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
poly_reg= PolynomialFeatures(degree=2)
X_poly=poly_reg.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=1)

lin_reg=LinearRegression()
lin_reg.fit(X_train, y_train)

poly_reg_y_predicted = lin_reg.predict(X_test)
from sklearn.metrics import mean_squared_error
poly_reg_rmse = np.sqrt(mean_squared_error(y_test, poly_reg_y_predicted))
poly_reg_rmse

from sklearn.metrics import r2_score
 r2_score(y_test, poly_reg_y_predicted)

"""# classifing according to price

**nooooow, classification**

**we need to divide our data into classes so we can apply classification models on it.(we can divide according to any feature but price is the most realistic)**

**I choose to classify according to the price once (and one more with year built), so the point that the user will ask for the programm will tell him in which class his features lye.**
"""

sns.histplot(data=df, x="price", color="blue",bins=10)

"""**to choose our threshold we will use confustion matrix and classification report to search for the suitable threshold that gives f1 score,recal and percision nearer to 1 and good accuracy**"""

#choosing threshold (0.7) (we will check if it is the best value or not below)
quantiles = df['price'].quantile([0.7])
#labeling of groups
df['group'] = 0
df['group'][df['price'] < quantiles[0.7] ] = 1

#take a look on groups column
df['group'].unique()

df['group'].hist() #no values except 0 and 1

"""**now, the data is ready for classification as we have 'groups' target with discrete vales**"""

df.head(2)

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 41)

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)
# Predicting the Test set results
y_pred = classifier.predict(X_test)

"""**we need Confusion Matrix to help us calculate true positive and false positive rate -> so we can choose out thershold**"""

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(classifier, X_test, y_test)  
plt.show()

"""**roc curve helps us to know wich value to choose**"""

pip install plot-metric

from plot_metric.functions import BinaryClassification
# Visualisation with plot_metric
bc = BinaryClassification(y_test, y_pred, labels=["Class 1", "Class 2"])

# Figures
plt.figure(figsize=(5,5))
bc.plot_roc_curve()
plt.show()

from sklearn import metrics
#calculate AUC of model
auc = metrics.roc_auc_score(y_test, y_pred)

#print AUC score
print(auc)

"""**by trial the largest area (auc) is 0.7 as threshold**"""

from sklearn.metrics import classification_report
cr=classification_report(y_test, y_pred)
print(cr)

"""**trying another models**"""

from sklearn.neighbors import KNeighborsClassifier
for k in range(3,10,2):
      knn = KNeighborsClassifier(n_neighbors=k)

      knn.fit(X_train, y_train)

# Calculate the accuracy of the model
      print("k= ",k,"acc =", knn.score(X_test, y_test) * 100)

"""**logistic regressio accuracy is better**

# classifying according to year built

**i also will try to classify with the year the apartment was built**
"""

#first drop groups column
df.drop(columns='group',axis=1,inplace = True)

sns.histplot(data=df, x="yr_built", color="blue",bins=10)

#dividing ranges
quantiles = df['price'].quantile([0.7])
#labeling of groups
df['group'] = 0
df['group'][df['price'] < quantiles[0.7] ] = 1

df['group'].unique()

#selecting the new group column as target
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 41)

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(classifier, X_test, y_test)  
plt.show()

from plot_metric.functions import BinaryClassification
# Visualisation with plot_metric
bc = BinaryClassification(y_test, y_pred, labels=["Class 1", "Class 2"])

# Figures
plt.figure(figsize=(5,5))
bc.plot_roc_curve()
plt.show()

from sklearn import metrics
#calculate AUC of model
auc = metrics.roc_auc_score(y_test, y_pred)

#print AUC score
print(auc)

"""**0.7 is the best value**"""

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

from sklearn.metrics import classification_report
cr=classification_report(y_test, y_pred)
print(cr)

"""**tring another models**"""

from sklearn.neighbors import KNeighborsClassifier
for k in range(3,10,2):
      knn = KNeighborsClassifier(n_neighbors=k)

      knn.fit(X_train, y_train)

# Calculate the accuracy of the model
      print("k= ",k,"acc =", knn.score(X_test, y_test) * 100)

"""**also here logistic model is better**"""